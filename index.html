
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      <link rel="icon" href="assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.4.2, mkdocs-material-8.5.7">
    
    
      
        <title>IBM Developer Code Pattern</title>
      
    
    
      <link rel="stylesheet" href="assets/stylesheets/main.20d9efc8.min.css">
      
        
        <link rel="stylesheet" href="assets/stylesheets/palette.815d1a91.min.css">
        
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL(".",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="blue" data-md-color-accent="light-blue">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#summarize-a-video-or-audio-file-using-ibm-watson" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="." title="IBM Developer Code Pattern" class="md-header__button md-logo" aria-label="IBM Developer Code Pattern" data-md-component="logo">
      
  <img src="assets/logo.svg" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            IBM Developer Code Pattern
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Instructions
            
          </span>
        </div>
      </div>
    </div>
    
      <form class="md-header__option" data-md-component="palette">
        
          
          
          <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="blue" data-md-color-accent="light-blue"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
          
            <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6zm0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4zM7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/></svg>
            </label>
          
        
          
          
          <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="blue" data-md-color-accent="light-blue"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_2">
          
            <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3Z"/></svg>
            </label>
          
        
      </form>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/IBM/video-summarizer-using-watson" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  

<nav class="md-nav md-nav--primary md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="." title="IBM Developer Code Pattern" class="md-nav__button md-logo" aria-label="IBM Developer Code Pattern" data-md-component="logo">
      
  <img src="assets/logo.svg" alt="logo">

    </a>
    IBM Developer Code Pattern
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/IBM/video-summarizer-using-watson" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Instructions
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="." class="md-nav__link md-nav__link--active">
        Instructions
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#flow" class="md-nav__link">
    Flow
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#watch-the-video" class="md-nav__link">
    Watch the Video
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#steps" class="md-nav__link">
    Steps
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1-clone-the-repo" class="md-nav__link">
    1. Clone the repo
  </a>
  
    <nav class="md-nav" aria-label="1. Clone the repo">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#application-directory-structure" class="md-nav__link">
    Application Directory structure
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-create-watson-services" class="md-nav__link">
    2. Create Watson Services
  </a>
  
    <nav class="md-nav" aria-label="2. Create Watson Services">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21-create-watson-speech-to-text-service-on-ibm-cloud" class="md-nav__link">
    2.1. Create Watson Speech to Text service on IBM Cloud
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22-add-watson-speech-to-text-credentials-to-the-application" class="md-nav__link">
    2.2. Add Watson Speech to Text credentials to the application
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-run-the-application" class="md-nav__link">
    3. Run the Application
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-generate-summary-and-insights-from-the-data" class="md-nav__link">
    4. Generate summary and insights from the data
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-watson-speech-to-text-optimization" class="md-nav__link">
    5. Watson Speech to Text Optimization
  </a>
  
    <nav class="md-nav" aria-label="5. Watson Speech to Text Optimization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#speaker-labels-beta" class="md-nav__link">
    Speaker Labels (Beta)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#smart-formatting" class="md-nav__link">
    Smart formatting
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#end-of-phrase-silence-time" class="md-nav__link">
    End of phrase silence time
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#numeric-redaction-beta" class="md-nav__link">
    Numeric redaction (Beta)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#profanity-filtering-beta" class="md-nav__link">
    Profanity filtering (Beta)
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6-summarizer-models-optimization" class="md-nav__link">
    6. Summarizer Models Optimization
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#summary" class="md-nav__link">
    Summary
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#license" class="md-nav__link">
    License
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  <a href="https://github.com/IBM/video-summarizer-using-watson/edit/master/docs/README.md" title="Edit this page" class="md-content__button md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25Z"/></svg>
  </a>


<h1 id="summarize-a-video-or-audio-file-using-ibm-watson">Summarize a video or audio file using IBM Watson</h1>
<p>In this code pattern, you will learn to build an end to end framework for generating summaries &amp; insights from video and/or audio files using a combination of IBM and Open source technologies.</p>
<p>Its always beneficial if we can get a gist of the content without going through the entire data and the problem adds more complexity if the data is in the form of a video or audio file. In this code pattern you will learn about building a robust solution for analyzing the video or audio files to quickly generate meaningful summary &amp; insights using different Deep learning and Machine learning approaches. You will also learn about improving the readibility of the transcripts with IBM Watson Speech to Text speech recognition models, how to optimize the parameteres, train different speech to text models and learn about different state of the art language models used for summarizing the text. </p>
<p>When you have completed this code pattern, you will understand how to:</p>
<ul>
<li>Use Watson Speech to Text service to convert the human voice into the written word.</li>
<li>Transcribe video/audio with greater readibility by tuning the Watson Speech to Text parameters.</li>
<li>Generate summary, highlights &amp; insights using Transformer &amp; ML based models.</li>
<li>Visualize the results on the GUI for quick consumption and analysis.</li>
</ul>
<!--add an image in this path-->
<p><img alt="architecture" src="doc/source/images/architecture.png" /></p>
<!--Optionally, add flow steps based on the architecture diagram-->
<h2 id="flow">Flow</h2>
<ol>
<li>User uploads a video or audio file.</li>
<li>If it is a video, then audio is extracted from the video.</li>
<li>The audio is sent to Watson Speech to Text that transcribes the audio to text.</li>
<li>The text is processed to extract summary, keywords &amp; insights with different approaches.</li>
<li>The speaker diarization, summary and transcript are displayed on the UI.</li>
<li>User can then download the insights.</li>
</ol>
<!--Optionally, update this section when the video is created-->
<h2 id="watch-the-video">Watch the Video</h2>
<!-- [![video](http://img.youtube.com/vi/zEHNVXtspM0/0.jpg)](https://www.youtube.com/watch?v=zEHNVXtspM0) -->
<iframe src="https://www.youtube.com/embed/zEHNVXtspM0" frameborder="0" allowfullscreen width="560" height="315"></iframe>

<h2 id="steps">Steps</h2>
<ol>
<li><a href="#1-clone-the-repo">Clone the repo</a></li>
<li><a href="#2-create-watson-services">Create Watson Services</a><ul>
<li>2.1. <a href="#21-create-watson-speech-to-text-service-on-ibm-cloud">Create Watson Speech to Text service on IBM Cloud</a></li>
<li>2.2. <a href="#22-add-watson-speech-to-text-credentials-to-the-application">Add Watson Speech to Text credentials to the application</a></li>
</ul>
</li>
<li><a href="#3-run-the-application">Run the Application</a></li>
<li><a href="#4-generate-summary-and-insights-from-the-data">Generate summary and insights from the data</a></li>
<li><a href="#5-watson-speech-to-text-optimization">Watson Speech to Text Optimization</a></li>
<li><a href="#6-summarizer-models-optimization">Summarizer Models Optimization</a></li>
</ol>
<h2 id="1-clone-the-repo">1. Clone the repo</h2>
<p>Clone the <code>video-summarizer-using-watson</code> repo locally. In a terminal, run:</p>
<div class="highlight"><pre><span></span><code>git clone https://github.com/IBM/video-summarizer-using-watson.git
</code></pre></div>
<h3 id="application-directory-structure">Application Directory structure</h3>
<p>The Application is built on Python Flask Framework.</p>
<ul>
<li>
<p>The directory structure is as follows:</p>
<p><pre>
.
├── Dockerfile
├── LICENSE
├── Notebooks
│   ├── IBM Watson Speech to Text custom model training.ipynb
│   └── Summarize.ipynb
├── Procfile
├── README.md
├── apis
│   ├── <strong>init</strong>.py
│   ├── summarizer.py
│   ├── videoUtils.py
│   └── watsonSpeechToText.py
├── app.py
├── deploy.yaml
├── manifest.yml
├── requirements.txt
├── static
│   ├── audios
│   ├── chunks
│   ├── credentials
│   │   └── speechtotext.json
│   ├── css
│   │   └── style.css
│   ├── images
│   ├── js
│   │   └── script.js
│   ├── transcripts
│   └── videos
│       └── wc.png
└── templates
    └── index.html
</pre></p>
</li>
<li>
<p><code>apis/</code> contains the API endpoints.</p>
</li>
<li><code>/api/v1.0/uploadVideo</code>: This API is used to upload the video file, extract audio from the video file, detect long pauses in the audio file and split the audio file into chunks.</li>
<li><code>/api/v1.0/transcribe/&lt;string:model&gt;</code>: This API is used to transcribe the audio files using Watson Speech to Text.</li>
<li><code>/api/v1.0/summarize</code>: This API is used to summarize the text using GTP-2, Gensim and XLNET summarizers.</li>
<li><code>static/</code> contains the following static files.</li>
<li><code>credentials/</code> contains the credentials for Watson Speech to Text.</li>
<li><code>videos/</code> contains the uploaded video files.</li>
<li><code>audios/</code> contains the extracted audio files.</li>
<li><code>transcripts/</code> contains the transcribed text files.</li>
<li><code>chunks/</code> contains the audio chunks.</li>
<li><code>css/</code> contains the CSS files.</li>
<li><code>js/</code> contains the JavaScript files.</li>
<li><code>templates/</code> contains the HTML templates.</li>
<li><code>app.py</code> is the main application file to run the flask server.</li>
<li><code>Dockerfile</code> is the Dockerfile to build the Docker image.</li>
<li><code>requirements.txt</code> is the list of requirements for the application.</li>
<li><code>deploy.yaml</code> is the deployment configuration file.</li>
</ul>
<h2 id="2-create-watson-services">2. Create Watson Services</h2>
<h3 id="21-create-watson-speech-to-text-service-on-ibm-cloud">2.1. Create Watson Speech to Text service on IBM Cloud</h3>
<ul>
<li>
<p>Login to IBM Cloud, create a <a href="https://cloud.ibm.com/catalog/services/speech-to-text">Watson Speech To Text Service</a>, and click on <code>create</code> as shown.
<img alt="Speech-to-text-service" src="doc/source/images/stt-service.png" /></p>
</li>
<li>
<p>In Speech To Text Dashboard, Click on <code>Services Credentials</code>.
<img alt="" src="doc/source/images/service-credentials.png" /></p>
</li>
<li>
<p>Click on <code>New credential</code> and add a service credential as shown.
<img alt="" src="doc/source/images/create-stt-credentials.gif" /></p>
</li>
<li>
<p>Copy the credentials.</p>
</li>
</ul>
<h3 id="22-add-watson-speech-to-text-credentials-to-the-application">2.2. Add Watson Speech to Text credentials to the application</h3>
<ul>
<li>
<p>Add the Watson Speech to Text credentials in the <code>static/credentials/speechtotext.json</code> file.</p>
<div class="highlight"><pre><span></span><code><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="nt">&quot;apikey&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;xxxx&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">    </span><span class="nt">&quot;iam_apikey_description&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;xxxx&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">    </span><span class="nt">&quot;iam_apikey_name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;xxxx&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">    </span><span class="nt">&quot;iam_role_crn&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;xxxx&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">    </span><span class="nt">&quot;iam_serviceid_crn&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;xxxx&quot;</span><span class="p">,</span><span class="w"></span>
<span class="w">    </span><span class="nt">&quot;url&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;xxxx&quot;</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</code></pre></div>
</li>
</ul>
<h2 id="3-run-the-application">3. Run the Application</h2>
<p>You can choose to run the application Locally or deploy on Red Hat OpenShift or deploy on IBM Public Cloud Foundry.</p>
<details><summary><b>Locally</b></summary>

* Navigate to the root of the cloned repo. In terminal, run the following command:

    <div class="highlight"><pre><span></span><code>docker build -t video-summarizer-using-watson:v1.0 .
</code></pre></div>

* Run the application locally. In terminal, run the following command:

    <div class="highlight"><pre><span></span><code>docker run -p <span class="m">8080</span>:8080 video-summarizer-using-watson:v1.0
</code></pre></div>

* Visit <http://localhost:8080> to see the application.

</details>

<details><summary><b>Red Hat OpenShift</b></summary>

### Steps to Build and Deploy on OpenShift

#### Build

> Note: Make sure you have docker cli installed and logged in to DockerHub

* In cloned repo, build the docker image. In terminal run:

    <div class="highlight"><pre><span></span><code>docker build -t &lt;your-docker-username&gt;/video-summarizer-using-watson:v1.0 .
</code></pre></div>

    > Replace `<your-docker-username>` with your docker hub username

* Once the docker image is built, deploy the docker image to Dockerhub. In terminal run:

    <div class="highlight"><pre><span></span><code>docker push &lt;your-docker-username&gt;/video-summarizer-using-watson:v1.0
</code></pre></div>

* At this point you have built the container image and successfully pushed to to a container repository dockerhub.

* Copy the image tag `<your-docker-username>/video-summarizer-using-watson:v1.0` and replace it on line no `18` in [deploy.yaml](deploy.yaml)

    <pre><code>spec:
        containers:
        - name: video-summarizer-using-watson
            image:<b> < your-docker-username >/video-summarizer-using-watson:v1.0 </b>
            ports:
            - containerPort: 8080
    </code></pre>

#### Deploy

* Login to your OpenShift cluster, In terminal run:

    <div class="highlight"><pre><span></span><code>oc login -u &lt;username&gt; -p &lt;password&gt;
</code></pre></div>

* Alternatively you can also login with an auth token. Follow the [Step here](https://developer.ibm.com/tutorials/configure-a-red-hat-openshift-cluster-with-red-hat-marketplace/#4-connect-to-the-openshift-cluster-in-your-cli) to login through an auth token.

* Once you have logged into OpenShift from your terminal, you can run the `oc apply` command to deploy the Application on OpenShift. In cloned repo, navigate to `` directory and in terminal run:

    <div class="highlight"><pre><span></span><code>oc apply -f deploy.yaml
</code></pre></div>

    <div class="highlight"><pre><span></span><code>deployment.apps/video-summarizer-using-watson-deployment created
service/video-summarizer-using-watson-service created
</code></pre></div>
* Run the `oc get services` to get the service External URL.

   <div class="highlight"><pre><span></span><code>oc get services <span class="p">|</span> grep video-summarizer-using-watson-service
</code></pre></div>

   <div class="highlight"><pre><span></span><code>NAME                        TYPE           CLUSTER-IP       EXTERNAL-IP                            PORT<span class="o">(</span>S<span class="o">)</span>        AGE
video-summarizer-using-watson-service             LoadBalancer   <span class="m">172</span>.21.170.157   <span class="m">169</span>.60.236.228                         <span class="m">80</span>:32020/TCP   2m
</code></pre></div>

* At this point, you will have successfully deployed the Application on OpenShift.

* Visit **EXTERNAL-IP** for example: <http://169.60.236.228> to see the application.

</details>

<details><summary><b>IBM Public Cloud Foundry</b></summary>

### Steps to Build and Deploy on IBM Public Cloud Foundry

#### Build and Deploy

* Before you proceed, make sure you have installed [IBM Cloud CLI](https://cloud.ibm.com/docs/cli?topic=cloud-cli-getting-started&locale=en-US) in your deployment machine.

> Note: You need to set the `disk-quote` to be more than 2GB since pytorch library is huge and requires more than 2GB of disk space to get installed.

* From the cloned repo, in terminal, run the following commands to deploy the Application to IBM Cloud Foundry.

    * Log in to your IBM Cloud account, and select an API endpoint.

        <div class="highlight"><pre><span></span><code>ibmcloud login
</code></pre></div>

        >NOTE: If you have a federated user ID, instead use the following command to log in with your single sign-on ID.

        <div class="highlight"><pre><span></span><code>ibmcloud login --sso
</code></pre></div>

    * Target a Cloud Foundry org and space:

        <div class="highlight"><pre><span></span><code>ibmcloud target --cf
</code></pre></div>

    * From within the root of the cloned repo, push your app to IBM Cloud.

        <div class="highlight"><pre><span></span><code>ibmcloud cf push video-summarizer-using-watson
</code></pre></div>

* The [manifest.yml](manifest.yml) file will be used here to deploy the application to IBM Cloud Foundry.

* On Successful deployment of the application you will see something similar on your terminal as shown.

    <pre><code>
    Invoking 'cf push'...
    Shown below is a sample output

    Pushing from manifest to org abc@in.ibm.com / space dev as abc@in.ibm.com...

    ...

    Waiting for app to start...

    name:              video-summarizer-using-watson
    requested state:   started
    routes:            <b>video-summarizer-using-watson.xx-xx.mybluemix.net </b>
    last uploaded:     Sat 16 May 18:05:16 IST 2020
    stack:             cflinuxfs3
    buildpacks:        python

    type:            web
    instances:       1/1
    memory usage:    4G
    start command:   python app.py
        state     since                  cpu     memory           disk           details
    #0   <b>running</b>   2020-05-16T12:36:15Z   12.6%   116.5M of 4G   
    </code></pre>

* Once the app is deployed, from the output of the above command, you can visit the `routes` to launch the application.

* At this point, you will have successfully deployed the Application on IBM Cloud.

* Visit <http://video-summarizer-using-watson.xx-xx.mybluemix.net> to see the application.

</details>

<h2 id="4-generate-summary-and-insights-from-the-data">4. Generate summary and insights from the data</h2>
<ul>
<li>Upload any video/audio file. (.mp4/.mov or .mp3/.wav). You can use the dataset provided in the repo <a href="data/earnings-call-2019.mp4">data/earnings-call-2019.mp4</a> or <a href="data/earnings-call-Q-and-A.mp4">data/earnings-call-Q-and-A.mp4</a>
<img alt="screenshot1" src="doc/source/images/screenshot1.png" /></li>
</ul>
<details><summary><b>About the Dataset</b></summary>

For the code pattern demonstration, we have considered `IBM Earnings Call Q1 2019` Webex recording. The data has 20+ min of IBM Revenue discussion, and 2+ min of Q & A at the end of the recording. We have split the data into 2 parts:

- `earnings-call-2019.mp4` - (Duration - 24:40)
This is IBM revenue discussion meeting recording.

- `earnings-call-Q-and-A.mp4` - (Duration - 2:40)
This is a part of Q & A's asked at the end of the meeting.

</details>

<ul>
<li>
<p>Select the Watson Speech to Text Language and Acoustic Model.
<img alt="screenshot2" src="doc/source/images/screenshot2.png" /></p>
<blockquote>
<p>Custom language model is built to recognize the <strong>out of vocabulary</strong>  words from the audio. <a href="https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-languageCreate">Learn more</a></p>
<p>Custom accoustic model is built to recognize the <strong>accent</strong> of the speaker from the audio. <a href="https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-acoustic">Learn more</a></p>
<p>NOTE: A <strong>Standard account</strong> is required to train a custom Speech To Text Model. There are three types of plans, Lite (FREE), Standard and Premium (PAID) for more info visit <a href="https://cloud.ibm.com/catalog/services/speech-to-text">https://cloud.ibm.com/catalog/services/speech-to-text</a></p>
<p>You can refer to the <a href="Notebooks/IBM%20Watson%20Speech%20to%20Text%20custom%20model%20training.ipynb">IBM Watson Speech to Text custom model training.ipynb</a> notebook to learn in detail how to build and train custom Watson Speech to Text models.</p>
</blockquote>
</li>
<li>
<p>Click on submit.
<img alt="screenshot3" src="doc/source/images/screenshot3.png" /></p>
</li>
<li>
<p>It will take approximately the same amount of time as the duration of the video to process the Speaker Diarized Output, Summary and Transcript.</p>
</li>
<li>
<p>You can view the Speaker Diarized Output.
<img alt="screenshot4" src="doc/source/images/screenshot4.png" /></p>
</li>
</ul>
<blockquote>
<p>Speaker Diarization is a process of extracting multiple speakers information from an audio. <a href="https://en.wikipedia.org/wiki/Speaker_diarisation">Learn more</a></p>
</blockquote>
<ul>
<li>
<p>You can view the Summary from Gensim, GPT2 &amp; XLNet models which are ML &amp; Transformer based approaches respectively. The insights are generated using KeyBert model. You can refer to this Jupyter Notebook <a href="https://github.com/IBM/video-summarizer-using-watson/blob/main/Notebooks/Custom-Models-for-Summarization-and-Insights.ipynb">Custom-Models-for-Summarization-and-Insights.ipynb</a> and play around with the different settings of model hyperparameters to increase or decrease the size of the output to suit your requirements. 
<img alt="screenshot5-1" src="doc/source/images/screenshot5-1.png" />
<img alt="screenshot5-2" src="doc/source/images/screenshot5-2.png" /></p>
</li>
<li>
<p>You can also view the transcript.
<img alt="screenshot6" src="doc/source/images/screenshot6.png" /></p>
</li>
</ul>
<h2 id="5-watson-speech-to-text-optimization">5. Watson Speech to Text Optimization</h2>
<p>The Watson Speech to Text model can be optimized further to get more precise and accurate results. In this section you will learn about the following speech recognition parameters of the Watson Speech to Text:
- Speaker labels (Beta)
- Smart formatting
- End of phrase silence time
- Numeric redaction (Beta)
- Profanity filtering (Beta)</p>
<h3 id="speaker-labels-beta">Speaker Labels (Beta)</h3>
<p>Speaker labels parameter in Watson™ Speech to Text, identifies which person spoke which words in a conversation. It is best optimized for two person conversation scenario, however it can support upto 6 person but the performance may vary. </p>
<p>Example: Telephone conversation between two people, Q&amp;A between two people, etc.</p>
<p>To enable Speaker labels, add the <code>speaker_labels</code> parameter and set it to <code>true</code>.</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">Transcribe</span><span class="p">(</span><span class="n">audiofilepath</span><span class="p">):</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">audiofilepath</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">audio_file</span><span class="p">:</span>
        <span class="n">speech_recognition_results</span> <span class="o">=</span> <span class="n">speech_to_text</span><span class="o">.</span><span class="n">recognize</span><span class="p">(</span>
                <span class="n">audio</span><span class="o">=</span><span class="n">audio_file</span><span class="p">,</span>
                <span class="n">content_type</span><span class="o">=</span><span class="s1">&#39;audio/wav&#39;</span><span class="p">,</span>
                <span class="n">model</span><span class="o">=</span><span class="s1">&#39;en-US_NarrowbandModel&#39;</span><span class="p">,</span>
                <span class="n">speaker_labels</span><span class="o">=</span><span class="kc">True</span>
            <span class="p">)</span><span class="o">.</span><span class="n">get_result</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">speech_recognition_results</span>
</code></pre></div>
<blockquote>
<p>Note: We are using an <strong>US English NarrowBand Model</strong> to transcribe the audio. <strong><em>NarrowBand Model</em></strong> is optimized for human to human conversations, whereas, <strong><em>BroadBand Model</em></strong> is optimized for human to bot or vice-versa conversations. Change your model accordingly.</p>
<p>Note: Speaker Labels is already enabled in this code pattern. ✔️</p>
<p>Learn more about Speaker Labels <a href="https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-speaker-labels">here</a>.</p>
</blockquote>
<h3 id="smart-formatting">Smart formatting</h3>
<p>Smart formatting parameter in Watson™ Speech to Text, converts the following strings into more conventional representations:
- Dates
- Times
- Series of digits and numbers
- Phone numbers
- Currency values (for US English and Spanish)
- Internet email and web addresses (for US English and Spanish)</p>
<p>Example:</p>
<table>
<thead>
<tr>
<th>String Type</th>
<th>Smart formatting OFF ❌</th>
<th>Smart formatting ON ✔️</th>
</tr>
</thead>
<tbody>
<tr>
<td>Dates</td>
<td>I was born on the ninth of December nineteen hundred</td>
<td>I was born on 12/9/1900</td>
</tr>
<tr>
<td>Times</td>
<td>The meeting starts at nine thirty AM</td>
<td>The meeting starts at 9:30 AM</td>
</tr>
<tr>
<td>Numbers</td>
<td>The quantity is one million one hundred and one dollar</td>
<td>The quantity is $1000101</td>
</tr>
<tr>
<td>Phone numbers</td>
<td>Call me at nine one four two three seven one thousand</td>
<td>Call me at 914-237-1000</td>
</tr>
<tr>
<td>Internet email and web addresses</td>
<td>My email address is john dot doe at foo dot com</td>
<td>My email address is john.doe@foo.com</td>
</tr>
<tr>
<td>Combinations</td>
<td>The code is zero two four eight one and the date of service is May fifth two thousand and one</td>
<td>The code is 02481 and the date of service is 5/5/2001</td>
</tr>
</tbody>
</table>
<p>To enable Smart formatting, add the <code>smart_formatting</code> parameter and set it to <code>true</code>.</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">Transcribe</span><span class="p">(</span><span class="n">audiofilepath</span><span class="p">):</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">audiofilepath</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">audio_file</span><span class="p">:</span>
        <span class="n">speech_recognition_results</span> <span class="o">=</span> <span class="n">speech_to_text</span><span class="o">.</span><span class="n">recognize</span><span class="p">(</span>
                <span class="n">audio</span><span class="o">=</span><span class="n">audio_file</span><span class="p">,</span>
                <span class="n">content_type</span><span class="o">=</span><span class="s1">&#39;audio/wav&#39;</span><span class="p">,</span>
                <span class="n">model</span><span class="o">=</span><span class="s1">&#39;en-US_NarrowbandModel&#39;</span><span class="p">,</span>
                <span class="n">speaker_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">smart_formatting</span><span class="o">=</span><span class="kc">True</span>
            <span class="p">)</span><span class="o">.</span><span class="n">get_result</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">speech_recognition_results</span>
</code></pre></div>
<blockquote>
<p>Note: Speaker Labels is already enabled in this code pattern. ✔️</p>
<p>Learn more about Smart formatting <a href="https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-formatting">here</a>.</p>
</blockquote>
<h3 id="end-of-phrase-silence-time">End of phrase silence time</h3>
<p>End of phrase silence time parameter in Watson™ Speech to Text, specifies the duration of the pause interval at which the transcript has to be split. This parameter improves the sentence formation in the transcript.</p>
<p>Example:</p>
<p>If a person speaks numbers such as one two three four with a pause between three and four, the transcript would be something like this if the End of phrase silence time not set:</p>
<p>"One two three"</p>
<p>"four"</p>
<p>However if the End of phrase silence time is set to say 1sec or 1.5sec then the transcript would be something like this:</p>
<p>"One two three four"</p>
<p>To enable End of phrase silence time, add the <code>end_of_phrase_silence_time</code> parameter and set it to desired time for example <code>1.5</code>sec.</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">Transcribe</span><span class="p">(</span><span class="n">audiofilepath</span><span class="p">):</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">audiofilepath</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">audio_file</span><span class="p">:</span>
        <span class="n">speech_recognition_results</span> <span class="o">=</span> <span class="n">speech_to_text</span><span class="o">.</span><span class="n">recognize</span><span class="p">(</span>
                <span class="n">audio</span><span class="o">=</span><span class="n">audio_file</span><span class="p">,</span>
                <span class="n">content_type</span><span class="o">=</span><span class="s1">&#39;audio/wav&#39;</span><span class="p">,</span>
                <span class="n">model</span><span class="o">=</span><span class="s1">&#39;en-US_NarrowbandModel&#39;</span><span class="p">,</span>
                <span class="n">speaker_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">smart_formatting</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">end_of_phrase_silence_time</span><span class="o">=</span><span class="mf">1.5</span>
            <span class="p">)</span><span class="o">.</span><span class="n">get_result</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">speech_recognition_results</span>
</code></pre></div>
<blockquote>
<p>Note: End of phrase silence time is already enabled in this code pattern. ✔️</p>
<p>Learn more about End of phrase silence time <a href="https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-parsing#silence-time">here</a>.</p>
</blockquote>
<h3 id="numeric-redaction-beta">Numeric redaction (Beta)</h3>
<p>Numeric redaction parameter in Watson™ Speech to Text, masks the numeric data from final transcripts. It is a useful feature when dealing with PII data.</p>
<p>Example:</p>
<table>
<thead>
<tr>
<th>Numeric redaction OFF ❌</th>
<th>Numeric redaction ON ✔️</th>
</tr>
</thead>
<tbody>
<tr>
<td>my credit card number is four one four seven two nine one three one seven eight two seven nine two six</td>
<td>my credit card number is xxxx-xxxx-xxxx-7926</td>
</tr>
</tbody>
</table>
<p>To enable Numeric redaction, add the <code>redaction</code> parameter and and set it to <code>true</code>.</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">Transcribe</span><span class="p">(</span><span class="n">audiofilepath</span><span class="p">):</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">audiofilepath</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">audio_file</span><span class="p">:</span>
        <span class="n">speech_recognition_results</span> <span class="o">=</span> <span class="n">speech_to_text</span><span class="o">.</span><span class="n">recognize</span><span class="p">(</span>
                <span class="n">audio</span><span class="o">=</span><span class="n">audio_file</span><span class="p">,</span>
                <span class="n">content_type</span><span class="o">=</span><span class="s1">&#39;audio/wav&#39;</span><span class="p">,</span>
                <span class="n">model</span><span class="o">=</span><span class="s1">&#39;en-US_NarrowbandModel&#39;</span><span class="p">,</span>
                <span class="n">speaker_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">smart_formatting</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">end_of_phrase_silence_time</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span>
                <span class="n">redaction</span><span class="o">=</span><span class="kc">True</span>
            <span class="p">)</span><span class="o">.</span><span class="n">get_result</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">speech_recognition_results</span>
</code></pre></div>
<blockquote>
<p>Learn more about Numeric redaction <a href="https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-formatting#numeric-redaction">here</a>.</p>
</blockquote>
<h3 id="profanity-filtering-beta">Profanity filtering (Beta)</h3>
<p>Profanity filtering parameter in Watson™ Speech to Text, censors profanity from its results. It is enabled by default, you can disable the feature if you want the words in the output exactly as transcribed.</p>
<p>Example:</p>
<table>
<thead>
<tr>
<th>Profanity filtering OFF ❌</th>
<th>Profanity filtering ON ✔️</th>
</tr>
</thead>
<tbody>
<tr>
<td><em>Fword</em> you</td>
<td>**** you</td>
</tr>
</tbody>
</table>
<p>To disable Profanity filtering, add the <code>redaction</code> parameter and and set it to <code>false</code>.</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">Transcribe</span><span class="p">(</span><span class="n">audiofilepath</span><span class="p">):</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">audiofilepath</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">audio_file</span><span class="p">:</span>
        <span class="n">speech_recognition_results</span> <span class="o">=</span> <span class="n">speech_to_text</span><span class="o">.</span><span class="n">recognize</span><span class="p">(</span>
                <span class="n">audio</span><span class="o">=</span><span class="n">audio_file</span><span class="p">,</span>
                <span class="n">content_type</span><span class="o">=</span><span class="s1">&#39;audio/wav&#39;</span><span class="p">,</span>
                <span class="n">model</span><span class="o">=</span><span class="s1">&#39;en-US_NarrowbandModel&#39;</span><span class="p">,</span>
                <span class="n">speaker_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">smart_formatting</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">end_of_phrase_silence_time</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span>
                <span class="n">redaction</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">profanity_filter</span><span class="o">=</span><span class="kc">False</span>
            <span class="p">)</span><span class="o">.</span><span class="n">get_result</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">speech_recognition_results</span>
</code></pre></div>
<blockquote>
<p>Learn more about Profanity filtering <a href="https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-formatting#profanity-filtering">here</a>.</p>
</blockquote>
<h2 id="6-summarizer-models-optimization">6. Summarizer Models Optimization</h2>
<p>The approaches used for extractive summarization are based on Transformer &amp; Machine Learning.</p>
<p><code>Transformer</code></p>
<p>This approach pays equal attention to the words in the data and establish a relation between the words which are far from each other. Transformer approach works on attention mechanism to generate highly accurate and cohesive summary which can capture the context well and generate meaningful insights. </p>
<p><code>Hyperparameters</code></p>
<p>We can adjust the summary size by setting the ratio (0.1 till 1 where the ideal setting would be ratio=0.2). We can also control the summary size by using the num_sentences parameter (ex:- num_sentences=10) to manually update the number of sentences to be present in the summarized output. There are a lot of other parameters that can be adjusted for different tasks like classification, sentence generation etc and for summarization the above mentioned parameters should ideally suffice. We can also re-train the models if required by referring to the link below.</p>
<p>Learn more about Transformers <a href="https://huggingface.co/docs/transformers/index">here.</a></p>
<p><code>Machine Learning</code></p>
<p>This approaches uses <code>Gensim</code> module for generating extractive summary. It uses the text rank algorithm to select the sentences on basis of the ranking of sentences in a specific order. Its a light weight module and can run on any setup (Cloud or local). </p>
<p><code>Hyperparameters</code></p>
<p>We can control the summary size by using the ratio parameter (ex:- ratio=0.2) and word_count (ex:- word_count=250) parameter. This method is useful for generating quick summary and keywords and might need skilled inference to consume the output.</p>
<h2 id="summary">Summary</h2>
<p>In this code pattern you learned how to create an integrated system to convert speech to text, generate summary and insights from a video or audio file. You also learned about improving the readibility of the transcripts by tuning the parameters of Watson Speech to Text and finally you learned about different state of the art language models used for summarizing the text. </p>
<p>This solution has wider applicability across domains to gather insights quickly from different data formats. This solution will be beneficial for Developers, Data Scientists &amp; Architects to understand the Transformer based architecture &amp; Watson Speech to Text capabilities to embed them in different apps to solve complex business problems associated with unstructured data.</p>
<!-- keep this -->
<h2 id="license">License</h2>
<p>This code pattern is licensed under the Apache License, Version 2. Separate third-party code objects invoked within this code pattern are licensed by their respective providers pursuant to their own separate licenses. Contributions are subject to the <a href="https://developercertificate.org/">Developer Certificate of Origin, Version 1.1</a> and the <a href="https://www.apache.org/licenses/LICENSE-2.0.txt">Apache License, Version 2</a>.</p>
<p><a href="https://www.apache.org/foundation/license-faq.html#WhatDoesItMEAN">Apache License FAQ</a></p>





                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var tab,labels=set.querySelector(".tabbed-labels");for(tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

        </div>
        
          <a href="#" class="md-top md-icon" data-md-component="top" hidden>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
            Back to top
          </a>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": ".", "features": ["toc.integrate", "navigation.top", "content.tabs.link"], "search": "assets/javascripts/workers/search.16e2a7d4.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}}</script>
    
    
      <script src="assets/javascripts/bundle.8492ddcf.min.js"></script>
      
    
  </body>
</html>